# -*- coding: utf-8 -*-
"""Streamlite.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rp8ywUnPKH0WS7uLk3IC8abU5MUidVBi
"""

# streamlit_app.py


# ---------------------------
# Imports
# ---------------------------
import streamlit as st
import pandas as pd
import numpy as np
import joblib
import os
import matplotlib.pyplot as plt
import seaborn as sns

st.set_page_config(page_title="Mood Reboot — Digital Detox", layout="wide")
sns.set_theme(style="whitegrid")

# ---------------------------
# Paths (adjust only if your repo structure differs)
# ---------------------------
MODELS_DIR = "models"
DATA_DIR = "data"
   # expects repo root/data/

# fallback if running from repo root directly
if not os.path.isdir(MODELS_DIR):
    MODELS_DIR = os.path.join(os.getcwd(), "models")
if not os.path.isdir(DATA_DIR):
    DATA_DIR = os.path.join(os.getcwd(), "data")

# ---------------------------
# Load models & artifacts (safe load with friendly errors)
# ---------------------------
lr = None
scaler_reg = None
try:
    lr = joblib.load(os.path.join(MODELS_DIR, "linear_mood_change_model.joblib"))
    scaler_reg = joblib.load(os.path.join(MODELS_DIR, "scaler_reg.joblib"))
except Exception as e:
    st.error(f"Could not load regression model/scaler from '{MODELS_DIR}'.\nError: {e}")
    st.stop()

# Classification artifacts (optional)
clf = None
scaler_clf = None
le = None
if os.path.exists(os.path.join(MODELS_DIR, "logistic_difficulty_model.joblib")):
    try:
        clf = joblib.load(os.path.join(MODELS_DIR, "logistic_difficulty_model.joblib"))
        scaler_clf = joblib.load(os.path.join(MODELS_DIR, "scaler_clf.joblib"))
        le = joblib.load(os.path.join(MODELS_DIR, "label_encoder_difficulty.joblib"))
    except Exception as e:
        st.warning("Classification artifacts found but couldn't be loaded. Classification will be disabled.")
        st.write(e)

# ---------------------------
# UI Layout
# ---------------------------
st.subheader("Model Predictions")
col1, col2 = st.columns(2)

# Prepare input dataframe (columns named exactly like your CSV)
# ---------------- Sidebar inputs (define these FIRST) ----------------
# ---------------- Sidebar inputs (define these FIRST) ----------------
# ---------------- Sidebar inputs (define these FIRST) ----------------
# ---------------- Sidebar inputs ----------------
# ---------------- Robust prediction: ensure exact feature order ----------------
import numpy as np

# Map slider variables to a simple dict (we had them defined already)
slider_map = {
    "Detox Duration": int(duration),
    "Baseline Mood": int(baseline_mood),
    "Baseline Stress": int(baseline_stress),
    "Baseline Sleep": int(baseline_sleep),
    "Baseline Focus": int(baseline_focus),
    "Screen Time": int(screen_time),
    "Sleep Hours": int(sleep_hours)
}

# Try to get the feature order from the scaler or model (preferred)
expected_features = None
if hasattr(scaler_reg, "feature_names_in_"):
    expected_features = list(scaler_reg.feature_names_in_)
elif hasattr(lr, "feature_names_in_"):
    expected_features = list(lr.feature_names_in_)

# If not found, fall back to a safe hardcoded order (edit if your training used something else)
if expected_features is None:
    expected_features = [
        "Detox Duration",
        "Baseline Mood",
        "Baseline Stress",
        "Baseline Sleep",
        "Baseline Focus",
        "Screen Time",
        "Sleep Hours"
    ]

# Show expected features in the app (helps debug if names mismatch)
st.info(f"Model expects features (in order): {expected_features}")

# Build input vector in the exact expected order, filling missing features with 0
input_values = []
missing = []
for feat in expected_features:
    if feat in slider_map:
        input_values.append(slider_map[feat])
    else:
        # if feature isn't provided by sliders (but model expected it), default to 0
        input_values.append(0)
        missing.append(feat)

if missing:
    st.warning(f"The following expected features were not provided by sliders and set to 0: {missing}")

# Create dataframe row with correct column names & order
X_reg = pd.DataFrame([input_values], columns=expected_features)

# Scale & predict safely with try/except
try:
    X_reg_scaled = scaler_reg.transform(X_reg)
    pred_mood_change = float(lr.predict(X_reg_scaled)[0])
    st.metric(label="Predicted Mood Improvement (0–10)", value=f"{pred_mood_change:.2f}")
    st.write("Predicted Post Mood:", round(min(10, baseline_mood + pred_mood_change), 2))
except Exception as e:
    st.error("Prediction failed — feature names/order mismatch. See logs. Error: " + str(e))

# Classification if available (uses same expected_features; change if classifier used different features)
if clf is not None and scaler_clf is not None and le is not None:
    try:
        # if classifier has its own feature_names_in_, use that
        clf_expected = None
        if hasattr(scaler_clf, "feature_names_in_"):
            clf_expected = list(scaler_clf.feature_names_in_)
        elif hasattr(clf, "feature_names_in_"):
            clf_expected = list(clf.feature_names_in_)
        else:
            clf_expected = expected_features  # assume same

        # build classifier input according to clf_expected
        X_clf = pd.DataFrame([{f: slider_map.get(f, 0) for f in clf_expected}])
        X_clf_scaled = scaler_clf.transform(X_clf)
        pred_class = clf.predict(X_clf_scaled)[0]
        pred_label = le.inverse_transform([pred_class])[0]
        st.metric(label="Predicted Detox Difficulty", value=pred_label)
    except Exception as e:
        st.warning("Classification failed (feature mismatch). Error: " + str(e))
else:
    st.info("Classification model not available or artifacts missing.")

# Regression input features used during training (match your training script)
reg_features = ["Detox Duration", "Stress Reduction", "Sleep Improvement", "Screen Time"]
X_reg = input_df[reg_features]

# Scale & predict (regression)
if lr is not None and scaler_reg is not None:
    X_reg_scaled = scaler_reg.transform(X_reg)
    pred_mood_change = lr.predict(X_reg_scaled)[0]

    # Display regression result
    with col1:
        st.metric(label="Predicted Mood Improvement (0–10)", value=f"{float(pred_mood_change):.2f}")
        st.write("Predicted Post Mood (Baseline + Improvement):", min(10, baseline_mood + pred_mood_change).__round__(2))
else:
    with col1:
        st.warning("Regression model or scaler could not be loaded. Cannot predict mood improvement.")


# Classification (if available)
with col2:
    if clf is not None and scaler_clf is not None and le is not None:
        X_clf_scaled = scaler_clf.transform(X_reg)
        pred_class = clf.predict(X_clf_scaled)[0]
        pred_label = le.inverse_transform([pred_class])[0]
        st.metric(label="Predicted Detox Difficulty", value=pred_label)
    else:
        st.info("Classification model not available. Upload `logistic_difficulty_model.joblib`, `scaler_clf.joblib`, and `label_encoder_difficulty.joblib` into the models/ folder.")

# ---------------------------
# Bottom: Data & Visuals
# ---------------------------
st.markdown("---")
st.subheader("Dataset Summary & Visuals")

# Try to load dataset if exists
data_loaded = False
csv_path_candidates = [
    os.path.join(DATA_DIR, "Digital_Detoxx_Final_RealBalanced.csv"),
    os.path.join(DATA_DIR, "Digital_Detoxx_Final_200.csv"),
    os.path.join(DATA_DIR, "Digital_Detoxx_Realistic_Accuracy.csv"),
    os.path.join(DATA_DIR, "Digital_Detoxx_Final_RealBalanced.csv"),
    os.path.join(DATA_DIR, "Digital_Detoxx_Final.csv"),
]
df = None
for p in csv_path_candidates:
    if os.path.exists(p):
        df = pd.read_csv(p)
        data_loaded = True
        break

if not data_loaded:
    st.warning("Dataset not found in data/ folder. Place your CSV (final dataset) inside 'data/' for dataset visuals.")
else:
    st.write(f"Loaded dataset: `{os.path.basename(p)}` — shape: {df.shape}")
    st.markdown("**Head of dataset:**")
    st.dataframe(df.head())

    # Simple stats
    with st.expander("Show summary statistics"):
        st.dataframe(df.describe(include='all').T)

    # Charts
    fig, ax = plt.subplots(1, 2, figsize=(12,4))
    sns.boxplot(x=df["Difficulty Level"], y=df["Stress Reduction"], ax=ax[0])
    ax[0].set_title("Stress Reduction by Difficulty Level")
    sns.histplot(df["Mood Change"], bins=12, ax=ax[1])
    ax[1].set_title("Distribution of Mood Change")
    st.pyplot(fig)

    # Correlation heatmap (numeric only)
    num_cols = df.select_dtypes(include=["number"]).columns.tolist()
    if len(num_cols) >= 3:
        fig2, ax2 = plt.subplots(figsize=(8,6))
        sns.heatmap(df[num_cols].corr(), annot=True, fmt=".2f", cmap="coolwarm", center=0, ax=ax2)
        st.pyplot(fig2)

# ---------------------------
# Footer: How to use / Troubleshooting
# ---------------------------
st.markdown("---")
st.markdown("""
**How to use this app in your repo**

1. Put your models in the repo root `models/` folder:
   - `linear_mood_change_model.joblib`
   - `scaler_reg.joblib`
   - `logistic_difficulty_model.joblib` (optional)
   - `scaler_clf.joblib` (optional)
   - `label_encoder_difficulty.joblib` (optional)

2. Put your final CSV in `data/` (recommended filename: `Digital_Detoxx_Final_RealBalanced.csv`).

3. From repo root run:
   `streamlit run dashboard/streamlit_app.py`

**If the app cannot find models or data, check file paths and ensure the files are in the correct folders.**
""")
